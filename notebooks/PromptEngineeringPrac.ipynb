{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40e61d4a-b31c-4f8d-b842-4293ec7030aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Install dependencies and configure environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba068439-2383-4bf1-aab8-18cc3fa208b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks_genai_inference==0.2.3\n  Downloading databricks_genai_inference-0.2.3-py3-none-any.whl.metadata (9.6 kB)\nRequirement already satisfied: pyyaml>=5.4.1 in /databricks/python3/lib/python3.12/site-packages (from databricks_genai_inference==0.2.3) (6.0.1)\nRequirement already satisfied: requests<3,>=2.26.0 in /databricks/python3/lib/python3.12/site-packages (from databricks_genai_inference==0.2.3) (2.32.2)\nCollecting databricks-sdk==0.19.1 (from databricks_genai_inference==0.2.3)\n  Downloading databricks_sdk-0.19.1-py3-none-any.whl.metadata (34 kB)\nRequirement already satisfied: pydantic>=2.4.2 in /databricks/python3/lib/python3.12/site-packages (from databricks_genai_inference==0.2.3) (2.8.2)\nRequirement already satisfied: typing-extensions>=4.7.1 in /databricks/python3/lib/python3.12/site-packages (from databricks_genai_inference==0.2.3) (4.11.0)\nCollecting tenacity==8.2.3 (from databricks_genai_inference==0.2.3)\n  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\nCollecting httpx<1,>=0.23.0 (from databricks_genai_inference==0.2.3)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk==0.19.1->databricks_genai_inference==0.2.3) (2.38.0)\nRequirement already satisfied: anyio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->databricks_genai_inference==0.2.3) (4.2.0)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->databricks_genai_inference==0.2.3) (2024.6.2)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->databricks_genai_inference==0.2.3)\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->databricks_genai_inference==0.2.3) (3.7)\nCollecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->databricks_genai_inference==0.2.3)\n  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=2.4.2->databricks_genai_inference==0.2.3) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=2.4.2->databricks_genai_inference==0.2.3) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.26.0->databricks_genai_inference==0.2.3) (2.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.26.0->databricks_genai_inference==0.2.3) (2.2.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk==0.19.1->databricks_genai_inference==0.2.3) (5.3.3)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk==0.19.1->databricks_genai_inference==0.2.3) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk==0.19.1->databricks_genai_inference==0.2.3) (4.9)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->databricks_genai_inference==0.2.3) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk==0.19.1->databricks_genai_inference==0.2.3) (0.4.8)\nDownloading databricks_genai_inference-0.2.3-py3-none-any.whl (17 kB)\nDownloading databricks_sdk-0.19.1-py3-none-any.whl (447 kB)\nDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\nDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\nDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\nDownloading h11-0.16.0-py3-none-any.whl (37 kB)\nInstalling collected packages: tenacity, h11, httpcore, httpx, databricks-sdk, databricks_genai_inference\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.2.2\n    Not uninstalling tenacity at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-95b31a76-ab3a-4378-a703-6d4bb58d9e9a\n    Can't uninstall 'tenacity'. No files were found to uninstall.\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Not uninstalling h11 at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-95b31a76-ab3a-4378-a703-6d4bb58d9e9a\n    Can't uninstall 'h11'. No files were found to uninstall.\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.49.0\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-95b31a76-ab3a-4378-a703-6d4bb58d9e9a\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-connect 16.4.2 requires databricks-sdk>=0.46.0, but you have databricks-sdk 0.19.1 which is incompatible.\nmlflow-skinny 2.21.3 requires databricks-sdk<1,>=0.20.0, but you have databricks-sdk 0.19.1 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed databricks-sdk-0.19.1 databricks_genai_inference-0.2.3 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 tenacity-8.2.3\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install databricks_genai_inference==0.2.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0238c537-2f56-4ee8-a273-9b1399e19d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Query foundation models**\n",
    "**Query a chat completion model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1570245a-21b1-4105-97f0-ec0395cd34e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Mixture of Experts (MoE) model is a type of machine learning architecture that was first introduced in the 1990s. It's a relatively old but still relevant and powerful idea.\n\nThe basic concept behind a Mixture of Experts model is to combine the predictions of multiple \"experts\" to produce a single final output. Each expert is a separate model that is responsible for a specific sub-problem or subset of the input data. The MoE model learns to weight the contributions of each expert, assigning a higher weight to the expert that is most relevant for a given input.\n\nHere's a breakdown of how MoE works:\n\n1. **Experts**: Each expert is a separate model that takes a subset of the input data and produces a prediction or output.\n2. **Gating network**: The gating network takes the input data and outputs a set of weights, each associated with one of the experts.\n3. **Weighted sum**: The predictions from each expert are weighted by their corresponding weights output by the gating network, and the weighted sum is computed.\n\nThe overall MoE model can be represented as a combination of two networks:\n\nmoE(x) = Î£(w_k \\* e_k(x)), where w_k is the weight of each expert k, and e_k(x) is the prediction of each expert k for input x.\n\nMoE models have several advantages:\n\n1. **Flexibility**: MoE models can handle a wide range of tasks and data types.\n2. **Interpretability**: The weights of each expert provide insight into which sub-problem is most relevant for a given input.\n3. **Improved generalization**: By combining multiple experts, MoE models can learn to generalize to regions of the input space where single models might fail.\n\nHowever, MoE models also come with some drawbacks:\n\n1. **Increased complexity**: MoE models require training multiple models (experts) and a gating network.\n2. **Training complexity**: Training MoE models can be computationally expensive and require careful tuning of hyperparameters.\n\nOverall, Mixture of Experts models are a powerful tool for tackling complex, high-dimensional tasks that involve multiple sub-problems or subset of input data.\n"
     ]
    }
   ],
   "source": [
    "from databricks_genai_inference import ChatCompletion\n",
    "chat_response = ChatCompletion.create(model=\"databricks-meta-llama-3-1-8b-instruct\",\n",
    "                                      messages=[{\"role\": \"user\", \"content\": \"What is a mixture of experts model?\"},\n",
    "                                                {\"role\":\"system\", \"content\": \"You are a helpful assistant.\"}])\n",
    "print(chat_response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5a7fd9-ae56-456e-832b-8db492fdab8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n  \"id\": \"chatcmpl_5ee2fb0a-1e7e-468e-8fcc-3524f60b8d63\",\n  \"object\": \"chat.completion\",\n  \"created\": 1758710934,\n  \"model\": \"meta-llama-3.1-8b-instruct-110524\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"A Mixture of Experts (MoE) model is a type of machine learning architecture that was first introduced in the 1990s. It's a relatively old but still relevant and powerful idea.\\n\\nThe basic concept behind a Mixture of Experts model is to combine the predictions of multiple \\\"experts\\\" to produce a single final output. Each expert is a separate model that is responsible for a specific sub-problem or subset of the input data. The MoE model learns to weight the contributions of each expert, assigning a higher weight to the expert that is most relevant for a given input.\\n\\nHere's a breakdown of how MoE works:\\n\\n1. **Experts**: Each expert is a separate model that takes a subset of the input data and produces a prediction or output.\\n2. **Gating network**: The gating network takes the input data and outputs a set of weights, each associated with one of the experts.\\n3. **Weighted sum**: The predictions from each expert are weighted by their corresponding weights output by the gating network, and the weighted sum is computed.\\n\\nThe overall MoE model can be represented as a combination of two networks:\\n\\nmoE(x) = \\u03a3(w_k \\\\* e_k(x)), where w_k is the weight of each expert k, and e_k(x) is the prediction of each expert k for input x.\\n\\nMoE models have several advantages:\\n\\n1. **Flexibility**: MoE models can handle a wide range of tasks and data types.\\n2. **Interpretability**: The weights of each expert provide insight into which sub-problem is most relevant for a given input.\\n3. **Improved generalization**: By combining multiple experts, MoE models can learn to generalize to regions of the input space where single models might fail.\\n\\nHowever, MoE models also come with some drawbacks:\\n\\n1. **Increased complexity**: MoE models require training multiple models (experts) and a gating network.\\n2. **Training complexity**: Training MoE models can be computationally expensive and require careful tuning of hyperparameters.\\n\\nOverall, Mixture of Experts models are a powerful tool for tackling complex, high-dimensional tasks that involve multiple sub-problems or subset of input data.\"\n      },\n      \"finish_reason\": \"stop\",\n      \"logprobs\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 29,\n    \"completion_tokens\": 443,\n    \"total_tokens\": 472\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ed0d3db-b38b-424c-a0f5-357f5c116d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PromptEngineeringPrac",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
